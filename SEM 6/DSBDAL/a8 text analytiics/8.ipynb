{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"I will walk 500 miles and I would walk 500 more. Just to be the man who walks \" + \\\n",
    "            \"a thousand miles to fall down at your door!\"\n",
    "sentence2 = \"I played the play playfully as the players were playing in the play with playfullness\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\vaishnavi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\vaishnavi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\vaishnavi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\vaishnavi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vaishnavi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vaishnavi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Tokenized words: ['I', 'will', 'walk', '500', 'miles', 'and', 'I', 'would', 'walk', '500', 'more', '.', 'Just', 'to', 'be', 'the', 'man', 'who', 'walks', 'a', 'thousand', 'miles', 'to', 'fall', 'down', 'at', 'your', 'door', '!']\n",
      "\n",
      "Tokenized sentences: ['I will walk 500 miles and I would walk 500 more.', 'Just to be the man who walks a thousand miles to fall down at your door!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "print('Tokenized words:', word_tokenize(sentence1))\n",
    "print('\\nTokenized sentences:', sent_tokenize(sentence1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging Parts of Speech: [('I', 'PRP'), ('will', 'MD'), ('walk', 'VB'), ('500', 'CD'), ('miles', 'NNS'), ('and', 'CC'), ('I', 'PRP'), ('would', 'MD'), ('walk', 'VB'), ('500', 'CD'), ('more', 'JJR'), ('.', '.'), ('Just', 'NNP'), ('to', 'TO'), ('be', 'VB'), ('the', 'DT'), ('man', 'NN'), ('who', 'WP'), ('walks', 'VBZ'), ('a', 'DT'), ('thousand', 'NN'), ('miles', 'NNS'), ('to', 'TO'), ('fall', 'VB'), ('down', 'RP'), ('at', 'IN'), ('your', 'PRP$'), ('door', 'NN'), ('!', '.'), ('I', 'PRP'), ('played', 'VBD'), ('the', 'DT'), ('play', 'NN'), ('playfully', 'RB'), ('as', 'IN'), ('the', 'DT'), ('players', 'NNS'), ('were', 'VBD'), ('playing', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('play', 'NN'), ('with', 'IN'), ('playfullness', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "token = word_tokenize(sentence1) + word_tokenize(sentence2)\n",
    "tagged = pos_tag(token)                 \n",
    "\n",
    "print(\"Tagging Parts of Speech:\", tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop-Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unclean version: ['I', 'will', 'walk', '500', 'miles', 'and', 'I', 'would', 'walk', '500', 'more', '.', 'Just', 'to', 'be', 'the', 'man', 'who', 'walks', 'a', 'thousand', 'miles', 'to', 'fall', 'down', 'at', 'your', 'door', '!']\n",
      "\n",
      "Cleaned version: ['I', 'walk', '500', 'miles', 'I', 'would', 'walk', '500', '.', 'Just', 'man', 'walks', 'thousand', 'miles', 'fall', 'door', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "token = word_tokenize(sentence1)\n",
    "cleaned_token = []\n",
    "\n",
    "for word in token:\n",
    "    if word not in stop_words:\n",
    "        cleaned_token.append(word)\n",
    "\n",
    "print('Unclean version:', token)\n",
    "print('\\nCleaned version:', cleaned_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i play the play play as the player were play in the play with playful\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "token = word_tokenize(sentence2)\n",
    "\n",
    "stemmed = [stemmer.stem(word) for word in token]\n",
    "print(\" \".join(stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I played the play playfully a the player were playing in the play with playfullness\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "token = word_tokenize(sentence2)\n",
    "\n",
    "lemmatized_output = [lemmatizer.lemmatize(word) for word in token]\n",
    "print(\" \".join(lemmatized_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Certainly! Let's go through each step in detail:\n",
    "\n",
    "# Tokenization\n",
    "# Tokenization is the process of breaking down text into individual words or sentences, referred to as tokens. In this code snippet, the word_tokenize function from NLTK (Natural Language Toolkit) is used to tokenize the input sentences.\n",
    "\n",
    "# word_tokenize(sentence1) tokenizes the words in sentence1 and returns a list of individual words.\n",
    "# sent_tokenize(sentence1) tokenizes the sentences in sentence1 and returns a list of individual sentences.\n",
    "# POS Tagging\n",
    "# POS (Part-of-Speech) tagging is the process of assigning grammatical tags to words based on their role and context in a sentence. The pos_tag function from NLTK is used to perform POS tagging on the tokens obtained from the previous step.\n",
    "\n",
    "# pos_tag(token) takes a list of tokens as input and returns a list of tuples, where each tuple contains a word and its corresponding POS tag.\n",
    "# Stop-Words Removal\n",
    "# Stop words are commonly used words (e.g., \"the\", \"is\", \"in\") that do not carry significant meaning and are often removed from text during preprocessing. The NLTK library provides a predefined list of stop words for different languages. In this step, stop words are removed from the tokens obtained from the first step.\n",
    "\n",
    "# stopwords.words('english') retrieves the list of English stop words from NLTK.\n",
    "# The tokenized sentence is iterated, and each word is checked against the stop word list. If the word is not a stop word, it is added to the cleaned_token list.\n",
    "# Stemming\n",
    "# Stemming is the process of reducing words to their base or root form (e.g., \"playing\" to \"play\"). In this step, the Porter stemmer algorithm from NLTK is used to perform stemming on the tokens obtained from the input sentence.\n",
    "\n",
    "# PorterStemmer() creates an instance of the PorterStemmer class.\n",
    "# The tokenized sentence is iterated, and each word is stemmed using the stem() method of the stemmer object. The stemmed words are collected in the stemmed list.\n",
    "# Lemmatization\n",
    "# Lemmatization is the process of reducing words to their base or dictionary form (e.g., \"playing\" to \"play\"). In this step, the WordNet lemmatizer from NLTK is used to perform lemmatization on the tokens obtained from the input sentence.\n",
    "\n",
    "# WordNetLemmatizer() creates an instance of the WordNetLemmatizer class.\n",
    "# The tokenized sentence is iterated, and each word is lemmatized using the lemmatize() method of the lemmatizer object. The lemmatized words are collected in the lemmatized_output list.\n",
    "# Each step in the code performs a specific text preprocessing task, such as tokenization, POS tagging, stop-word removal, stemming, or lemmatization. These tasks are commonly used in natural language processing and text analysis to transform raw text into a more structured format that can be used for further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
